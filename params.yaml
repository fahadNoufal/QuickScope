model_finetuning:
  model_name: 'unsloth/Llama-3.2-1B-Instruct-bnb-4bit'
  max_seq_length: 1024
  device_map: 'auto'
  r: 32
  lora_alpha: 16
  lora_dropout: 0
  bias: 'none'
  use_gradient_checkpointing: 'unsloth'
  random_state: 3407
  use_rslora: false
  loftq_config: null

  # model_trainer
  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  dataset_text_field: 'text'
  warmup_steps: 5
  num_train_epochs: 2
  learning_rate: 2e-4
  logging_steps: 1
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  seed: 3407
  report_to: "none"

prediction:
  device_map: 'auto'
  max_new_tokens: 1024
  top_p: 0.9
  do_sample: true
  temperature: 1